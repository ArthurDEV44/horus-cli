# Horus CLI - Model Selection Guide

> **Guide complet** : S√©lection optimale des mod√®les Mistral/Ollama pour Horus CLI selon votre configuration mat√©rielle

**Derni√®re mise √† jour** : 2025-01-23 (Phase 5)
**Version** : 1.0

---

## Table des mati√®res

1. [Vue d'ensemble](#vue-densemble)
2. [Mod√®les disponibles](#mod√®les-disponibles)
3. [Matrice de s√©lection](#matrice-de-s√©lection)
4. [Configuration Ollama](#configuration-ollama)
5. [Utilisation](#utilisation)
6. [Trade-offs d√©taill√©s](#trade-offs-d√©taill√©s)
7. [FAQ](#faq)

---

## Vue d'ensemble

### Pourquoi la s√©lection de mod√®le est importante

Le **choix du mod√®le** est le facteur #1 de performance pour Horus CLI local :

- **Context window** : Plus grand = plus lent (scaling quadratique de l'attention)
- **Taille du mod√®le** : Plus gros = meilleure qualit√©, mais plus de VRAM et plus lent
- **VRAM disponible** : Limite physique contraignante pour les mod√®les locaux

### Recommandation par d√©faut

**üéØ mistral-small (22B, 32K context)** est le meilleur compromis pour la majorit√© des t√¢ches :
- Excellente qualit√© de code
- Contexte suffisant pour la plupart des refactors
- VRAM raisonnable (12-16 GB)
- Vitesse acceptable (4/5)

---

## Mod√®les disponibles

Horus CLI supporte 4 mod√®les Mistral principaux :

| Mod√®le | Taille | Context | VRAM Min | VRAM Recommand√© | Vitesse | Qualit√© |
|--------|--------|---------|----------|-----------------|---------|---------|
| **mistral** | 7B | 8K | 4 GB | 6 GB | ‚ö°‚ö°‚ö°‚ö°‚ö° (5/5) | ‚≠ê‚≠ê‚≠ê (3/5) |
| **mistral-small** | 22B | 32K | 12 GB | 16 GB | ‚ö°‚ö°‚ö°‚ö° (4/5) | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) |
| **mixtral** | 8x7B MoE | 32K | 24 GB | 32 GB | ‚ö°‚ö°‚ö° (3/5) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) |
| **devstral:24b** | 24B | 128K | 32 GB | 40 GB | ‚ö°‚ö° (2/5) | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) |

### Caract√©ristiques d√©taill√©es

#### mistral (7B)
- **Use cases** : Navigation fichiers, petites √©ditions, r√©ponses rapides
- **Avantages** : Tr√®s rapide (80-120 tokens/sec), faible VRAM
- **Limites** : Contexte limit√© (8K), qualit√© code moyenne
- **Quand utiliser** : T√¢ches simples, prototypage, machines limit√©es

#### mistral-small (22B) ‚≠ê **RECOMMAND√â**
- **Use cases** : Refactors multi-fichiers, analyses approfondies, most tasks
- **Avantages** : Excellent compromis qualit√©/vitesse, contexte g√©n√©reux (32K)
- **Limites** : VRAM significatif (12-16 GB)
- **Quand utiliser** : T√¢che par d√©faut, 80% des cas d'usage

#### mixtral (8x7B MoE)
- **Use cases** : Refactors complexes, d√©cisions d'architecture, subagents parall√®les
- **Avantages** : Meilleure qualit√© (5/5), architecture MoE efficace
- **Limites** : VRAM √©lev√© (24-32 GB), plus lent
- **Quand utiliser** : T√¢ches critiques n√©cessitant la meilleure qualit√©

#### devstral:24b (24B, 128K)
- **Use cases** : Longs contextes, sessions multi-heures, deep debugging
- **Avantages** : Contexte √©norme (128K), excellente compr√©hension
- **Limites** : Tr√®s lent, VRAM tr√®s √©lev√© (32-40 GB)
- **Quand utiliser** : Analyse de gros projets, sessions longues

---

## Matrice de s√©lection

### S√©lection automatique par VRAM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VRAM Disponible                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  < 8 GB   ‚îÇ  8-16 GB    ‚îÇ  16-32 GB   ‚îÇ  32+ GB             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ           ‚îÇ             ‚îÇ             ‚îÇ                     ‚îÇ
‚îÇ  mistral  ‚îÇ mistral-    ‚îÇ  mixtral    ‚îÇ   devstral          ‚îÇ
‚îÇ   (7B)    ‚îÇ  small      ‚îÇ  (8x7B)     ‚îÇ   (24B, 128K)       ‚îÇ
‚îÇ           ‚îÇ  (22B)      ‚îÇ             ‚îÇ                     ‚îÇ
‚îÇ           ‚îÇ             ‚îÇ             ‚îÇ                     ‚îÇ
‚îÇ  8K ctx   ‚îÇ  32K ctx    ‚îÇ  32K ctx    ‚îÇ   128K ctx          ‚îÇ
‚îÇ  ~1-2s    ‚îÇ  ~3-5s      ‚îÇ  ~8-12s     ‚îÇ   ~15-30s           ‚îÇ
‚îÇ  latence  ‚îÇ  latence    ‚îÇ  latence    ‚îÇ   latence           ‚îÇ
‚îÇ           ‚îÇ             ‚îÇ             ‚îÇ                     ‚îÇ
‚îÇ  Use:     ‚îÇ  Use:       ‚îÇ  Use:       ‚îÇ   Use:              ‚îÇ
‚îÇ  - Fast   ‚îÇ  - Most     ‚îÇ  - Complex  ‚îÇ   - Long sessions   ‚îÇ
‚îÇ    nav    ‚îÇ    tasks    ‚îÇ    refactor ‚îÇ   - Deep analysis   ‚îÇ
‚îÇ  - Small  ‚îÇ  - Medium   ‚îÇ  - Parallel ‚îÇ   - Multi-hour      ‚îÇ
‚îÇ    edits  ‚îÇ    refactor ‚îÇ    subagent ‚îÇ     debugging       ‚îÇ
‚îÇ           ‚îÇ             ‚îÇ                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### S√©lection par contexte requis

| Contexte requis | VRAM < 8GB | VRAM 8-16GB | VRAM 16-32GB | VRAM 32GB+ |
|-----------------|------------|-------------|--------------|------------|
| **< 8K tokens** | mistral | mistral-small | mistral-small | mistral-small |
| **8-16K tokens** | mistral | mistral-small | mistral-small | mistral-small |
| **16-32K tokens** | ‚ùå Error | mistral-small | mixtral | mixtral |
| **> 32K tokens** | ‚ùå Error | ‚ùå Error | ‚ùå Error | devstral:24b |

---

## Configuration Ollama

### Installation des mod√®les

```bash
# Installer tous les mod√®les Mistral
ollama pull mistral
ollama pull mistral-small
ollama pull mixtral
ollama pull devstral:24b
```

### Cr√©er des Modelfiles custom

**1. Mistral (fast profile)**

```bash
# ~/.ollama/models/horus-mistral.modelfile
FROM mistral

PARAMETER num_ctx 8192
PARAMETER temperature 0.2
PARAMETER top_p 0.9

SYSTEM """
You are Horus, a local AI coding assistant. You help developers by:
- Reading and analyzing code files
- Making precise edits using tools
- Explaining complex architectures
- Refactoring code for clarity

Always prefer small, focused changes over large rewrites.
"""
```

**2. Mistral-small (balanced profile)** ‚≠ê

```bash
# ~/.ollama/models/horus-mistral-small.modelfile
FROM mistral-small

PARAMETER num_ctx 32768
PARAMETER temperature 0.2
PARAMETER top_p 0.9

SYSTEM """
You are Horus, a local AI coding assistant. You help developers by:
- Reading and analyzing code files
- Making precise edits using tools
- Explaining complex architectures
- Refactoring code for clarity

Always prefer small, focused changes over large rewrites.
"""
```

**3. Mixtral (powerful profile)**

```bash
# ~/.ollama/models/horus-mixtral.modelfile
FROM mixtral

PARAMETER num_ctx 32768
PARAMETER temperature 0.2
PARAMETER top_p 0.9

SYSTEM """
You are Horus, a local AI coding assistant. You help developers by:
- Reading and analyzing code files
- Making precise edits using tools
- Explaining complex architectures
- Refactoring code for clarity

You excel at complex refactoring and architectural decisions.
"""
```

**4. Devstral (deep profile)**

```bash
# ~/.ollama/models/horus-devstral.modelfile
FROM devstral:24b

PARAMETER num_ctx 128000
PARAMETER temperature 0.2
PARAMETER top_p 0.9

SYSTEM """
You are Horus, a local AI coding assistant. You help developers by:
- Reading and analyzing code files
- Making precise edits using tools
- Explaining complex architectures
- Refactoring code for clarity

You have access to very large context windows for deep analysis.
"""
```

### Build custom models

```bash
ollama create horus-mistral -f ~/.ollama/models/horus-mistral.modelfile
ollama create horus-mistral-small -f ~/.ollama/models/horus-mistral-small.modelfile
ollama create horus-mixtral -f ~/.ollama/models/horus-mixtral.modelfile
ollama create horus-devstral -f ~/.ollama/models/horus-devstral.modelfile
```

### Configuration Horus CLI

```json
// .horus/settings.json
{
  "currentModel": "mistral-small",
  "models": {
    "mistral": {
      "maxContext": 8192,
      "provider": "ollama"
    },
    "mistral-small": {
      "maxContext": 32768,
      "provider": "ollama"
    },
    "mixtral": {
      "maxContext": 32768,
      "provider": "ollama"
    },
    "devstral:24b": {
      "maxContext": 128000,
      "provider": "ollama"
    }
  },
  "contextSettings": {
    "autoSelectModel": true,
    "reservedContextPercent": 0.3,
    "cacheEnabled": true
  }
}
```

---

## Utilisation

### Commande de benchmark

```bash
# Voir les recommandations pour votre syst√®me
horus context bench

# Tester un profil sp√©cifique
horus context bench --profile balanced

# Output JSON
horus context bench --json
```

**Exemple de sortie** :

```
üèãÔ∏è  System Benchmark & Model Recommendation

Detecting system configuration...
System Configuration:
  Platform: linux
  CPU Cores: 16
  RAM: 15.4 GB
  GPU Type: nvidia
  GPU Name: NVIDIA GeForce RTX 3090
  VRAM: 24 GB

Model Recommendations by Context Size:

Small Context (4K tokens):
  Model: mistral (fast)
  Reason: Small context, optimized for speed

Medium Context (16K tokens):
  Model: mistral-small (balanced)
  Reason: Small-medium context, optimal balance

Large Context (28K tokens):
  Model: mixtral (powerful)
  Reason: Medium-large context with high quality requirements

üí° Recommendations:

  ‚úì Your system can run mixtral (8x7B) for complex tasks
  ‚úì Recommended: Use mixtral for refactoring, mistral-small for most tasks
```

### Lancer Horus avec un mod√®le sp√©cifique

```bash
# Default (auto-s√©lectionne mistral-small)
horus

# Fast profile (mistral 7B)
horus --model mistral

# Balanced profile (mistral-small 22B) [RECOMMAND√â]
horus --model mistral-small

# Powerful profile (mixtral 8x7B)
horus --model mixtral

# Deep profile (devstral 128K)
horus --model devstral:24b
```

### Variables d'environnement

```bash
# Forcer un mod√®le sp√©cifique
export HORUS_MODEL=mistral-small

# Enable auto-selection (recommand√©)
export HORUS_AUTO_SELECT_MODEL=true

# Debug model selection
export HORUS_CONTEXT_DEBUG=true
```

---

## Trade-offs d√©taill√©s

### Comparaison chiffr√©e

| Crit√®re | mistral | mistral-small | mixtral | devstral |
|---------|---------|---------------|---------|----------|
| **Tokens/sec** | 80-120 | 40-60 | 20-30 | 10-15 |
| **Qualit√© code** | 7/10 | 9/10 | 9.5/10 | 9/10 |
| **Compr√©hension** | 7/10 | 9/10 | 10/10 | 9.5/10 |
| **Following instructions** | 8/10 | 9/10 | 9/10 | 8.5/10 |
| **Co√ªt compute (CPU)** | LOW | MEDIUM | HIGH | HIGH |
| **Co√ªt VRAM** | 5GB | 14GB | 28GB | 40GB |
| **Context window** | 8K | 32K | 32K | 128K |
| **Vitesse totale** | ~1-2s | ~3-5s | ~8-12s | ~15-30s |

### Quand utiliser chaque mod√®le

#### mistral (7B) - Fast Profile
‚úÖ **Utilisez quand** :
- Navigation rapide dans le code
- Petites √©ditions (1-2 fichiers)
- R√©ponses imm√©diates requises
- VRAM < 8GB

‚ùå **N'utilisez PAS quand** :
- Refactoring multi-fichiers
- Architecture complexe
- Contexte > 8K tokens requis

#### mistral-small (22B) - Balanced Profile ‚≠ê
‚úÖ **Utilisez quand** :
- Most tasks (80% des cas)
- Refactors multi-fichiers (jusqu'√† 10-15 fichiers)
- Analyses approfondies
- VRAM 12-16GB disponible

‚ùå **N'utilisez PAS quand** :
- VRAM < 12GB
- Contexte > 32K requis
- Vitesse critique (pr√©f√©rer mistral)

#### mixtral (8x7B) - Powerful Profile
‚úÖ **Utilisez quand** :
- Refactors complexes (architecture-wide)
- D√©cisions d'architecture
- Subagents parall√®les (max 3)
- Qualit√© maximale requise
- VRAM 24-32GB disponible

‚ùå **N'utilisez PAS quand** :
- VRAM < 24GB
- Vitesse critique
- T√¢ches simples (overkill)

#### devstral:24b (24B, 128K) - Deep Profile
‚úÖ **Utilisez quand** :
- Contexte > 32K requis
- Sessions multi-heures
- Deep debugging (analyser gros projets)
- VRAM 32GB+ disponible

‚ùå **N'utilisez PAS quand** :
- VRAM < 32GB
- Vitesse importante
- Contexte < 32K (overkill)

---

## FAQ

### Q1: Quel mod√®le choisir par d√©faut ?

**R**: **mistral-small (22B)** est le meilleur d√©faut pour 80% des cas. Il offre un excellent compromis qualit√©/vitesse.

### Q2: Mon syst√®me a seulement 8GB de RAM, quel mod√®le ?

**R**: Utilisez **mistral (7B)**. C'est le seul qui fonctionnera correctement avec 8GB de RAM totale (environ 4-6GB VRAM disponible).

### Q3: Comment am√©liorer la vitesse ?

**R**: 3 leviers principaux :
1. **R√©duire le contexte** : Shorter context = faster (scaling quadratique)
2. **Mod√®le plus petit** : mistral (7B) est 3-5x plus rapide que mixtral
3. **GPU upgrade** : Plus de VRAM permet des mod√®les plus efficaces

### Q4: Puis-je changer de mod√®le en cours de session ?

**R**: Non, le mod√®le est fix√© au d√©marrage. Utilisez `/exit` puis relancez Horus avec `--model <name>`.

### Q5: Comment savoir si j'ai assez de VRAM ?

**R**: Utilisez `horus context bench` pour voir les recommandations pour votre syst√®me.

### Q6: Mistral-small vs mixtral : quelle diff√©rence ?

**R**:
- **mistral-small** : 22B params, plus rapide (4/5), excellente qualit√© (4/5)
- **mixtral** : 8x7B MoE (56B params), plus lent (3/5), meilleure qualit√© (5/5)

Choisissez **mistral-small** par d√©faut, **mixtral** pour t√¢ches critiques.

### Q7: Le contexte de 128K de devstral est-il vraiment utilisable ?

**R**: Oui, mais tr√®s lent (~30s par r√©ponse). Utilisez seulement si vous avez vraiment besoin de 32K+ tokens de contexte (rare).

### Q8: Puis-je utiliser d'autres mod√®les que Mistral ?

**R**: Horus supporte tous les mod√®les Ollama compatibles OpenAI API. Ajoutez-les dans `model-configs.ts` pour la s√©lection automatique.

### Q9: Comment optimiser pour mon GPU sp√©cifique ?

**R**: Ajustez `num_ctx` dans le Modelfile :
- GPU < 8GB : `num_ctx 4096`
- GPU 8-16GB : `num_ctx 8192`
- GPU 16-32GB : `num_ctx 32768`
- GPU 32GB+ : `num_ctx 128000`

### Q10: Quel est le co√ªt en √©lectricit√© ?

**R**: Approximations (bas√©es sur RTX 3090, 350W TDP) :
- **mistral** : ~50W pendant g√©n√©ration (~0.05 kWh/h)
- **mistral-small** : ~150W pendant g√©n√©ration (~0.15 kWh/h)
- **mixtral** : ~280W pendant g√©n√©ration (~0.28 kWh/h)
- **devstral** : ~320W pendant g√©n√©ration (~0.32 kWh/h)

---

## Ressources suppl√©mentaires

### Documentation
- [ROADMAP.md Phase 5](../ROADMAP.md#phase-5--tuning-mod√®les--benchmarks)
- [CLAUDE.md - Model Selection](../CLAUDE.md#phase-5-tuning-mod√®les--benchmarks)
- [Ollama Model Library](https://ollama.com/library)

### Benchmarks
- [Mistral AI Benchmarks](https://mistral.ai/news/mistral-small/)
- [DeepSeek Coder Benchmarks](https://github.com/deepseek-ai/DeepSeek-Coder)
- [MODELE_CODING_BENCHMARKS.md](../MODELE_CODING_BENCHMARKS.md)

### Communaut√©
- [GitHub Issues](https://github.com/ArthurDEV44/horus-cli/issues)
- [Discussions](https://github.com/ArthurDEV44/horus-cli/discussions)

---

**Derni√®re r√©vision** : 2025-01-23 - Phase 5 compl√©t√©e
**Maintenu par** : √âquipe Horus CLI
**License** : MIT
