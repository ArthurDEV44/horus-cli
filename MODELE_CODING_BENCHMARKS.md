# üöÄ Mod√®les Ollama Optimis√©s pour le Coding (√âditeur CLI)

## üìã Contexte

Vous cherchez un mod√®le **puissant pour des t√¢ches de coding** similaires √† Cursor IDE, Claude Code, ou Codex. Ce document pr√©sente les meilleurs mod√®les Ollama sp√©cialis√©s en g√©n√©ration de code avec leurs benchmarks.

---

## üèÜ Top Mod√®les Recommand√©s pour le Coding

### ü•á **1. devstral** - Best Open Source Coding Agent

**Tags** : `tools 24b`  
**Pulls** : 410.1K  
**Taille** : 24B param√®tres  
**Description** : "the best open source model for coding agents"

**Points Forts** :
- ‚úÖ **Sp√©cialement con√ßu pour les agents de coding**
- ‚úÖ Support tool calling natif
- ‚úÖ Format OpenAI compatible
- ‚úÖ Optimis√© pour les t√¢ches complexes de d√©veloppement

**Commandes** :
```bash
ollama pull devstral:24b
```

**Benchmarks** :
- Con√ßu pour surpasser les mod√®les g√©n√©raux en t√¢ches de coding
- Optimis√© pour les workflows de d√©veloppement (refactoring, debugging, g√©n√©ration)

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **TOP CHOIX pour √©diteur CLI**

---

### ü•à **2. deepseek-coder-v2** - Competitive with GPT-4 Turbo

**Tags** : `16b 236b` (MoE)  
**Pulls** : 1.1M  
**Taille** : 16B activ√©s (236B total MoE)  
**Description** : "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks"

**Points Forts** :
- ‚úÖ **Performance comparable √† GPT-4 Turbo** sur t√¢ches de code
- ‚úÖ Architecture MoE (efficace)
- ‚úÖ Sp√©cialis√© code (entra√Æn√© sur 2 trillion tokens de code)
- ‚úÖ Support 80+ langages de programmation

**Commandes** :
```bash
ollama pull deepseek-coder-v2:16b    # Recommand√© (16B activ√©s)
ollama pull deepseek-coder-v2:236b   # Plus puissant (MoE complet)
```

**Benchmarks** :
- Surpasse CodeLlama et mod√®les similaires
- HumanEval : Scores √©lev√©s (> 70% pass@1)
- MBPP : Performances excellentes

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Excellente alternative**

---

### ü•â **3. qwen2.5-coder** - Latest Code-Specific Qwen

**Tags** : `tools 0.5b 1.5b 3b 7b 14b 32b`  
**Pulls** : 7.8M  
**Taille** : Multiple (7B, 14B, 32B recommand√©s)

**Points Forts** :
- ‚úÖ **Derni√®re s√©rie Code-Specific** de Qwen
- ‚úÖ Am√©liorations significatives en g√©n√©ration, raisonnement et correction de code
- ‚úÖ Support tool calling
- ‚úÖ Bon √©quilibre performance/taille

**Commandes** :
```bash
ollama pull qwen2.5-coder:7b     # √âquilibre taille/performance
ollama pull qwen2.5-coder:14b    # Plus puissant
ollama pull qwen2.5-coder:32b    # Maximum (si RAM suffisante)
```

**Benchmarks** :
- HumanEval : Scores comp√©titifs avec CodeLlama 7B+
- MBPP : Bonnes performances
- CodeXGLUE : Am√©liorations vs Qwen2.5 standard

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê **Excellent pour usage g√©n√©ral**

---

### üéØ **4. deepseek-coder** - Classic Code Specialist

**Tags** : `1.3b 6.7b 33b`  
**Pulls** : 1.5M  
**Taille** : 33B param√®tres (version compl√®te)  
**Description** : "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens"

**Points Forts** :
- ‚úÖ Entra√Æn√© sur **2 trillion tokens** de code et langage naturel
- ‚úÖ Sp√©cialis√© GitHub (corpus massif)
- ‚úÖ Support 80+ langages
- ‚úÖ Tr√®s populaire et bien test√©

**Commandes** :
```bash
ollama pull deepseek-coder:33b   # Version compl√®te (recommand√©e)
ollama pull deepseek-coder:6.7b  # Version plus l√©g√®re
```

**Benchmarks** (33B) :
- HumanEval : ~73% pass@1
- MBPP : ~76% pass@1
- Surpasse CodeLlama 34B et GPT-3.5 Turbo sur certaines t√¢ches

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê **Classique fiable**

---

### üî• **5. codestral** - Mistral AI Code Model

**Tags** : `22b`  
**Pulls** : 491.7K  
**Taille** : 22B param√®tres  
**Description** : "Codestral is Mistral AI's first-ever code model designed for code generation tasks"

**Points Forts** :
- ‚úÖ **Premier mod√®le code de Mistral AI**
- ‚úÖ Con√ßu sp√©cifiquement pour g√©n√©ration de code
- ‚úÖ Pr√©cis, rapide, faible empreinte m√©moire
- ‚úÖ Support 80+ langages
- ‚ö†Ô∏è **Licence restrictive** : Usage limit√© √† recherche/tests (pas commercial)

**Commandes** :
```bash
ollama pull codestral:22b
```

**Benchmarks** :
- Performances tr√®s √©lev√©es sur HumanEval
- Rapidit√© d'ex√©cution optimis√©e
- Compr√©hension contextuelle avanc√©e

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê (mais ‚ö†Ô∏è **licence restrictive**)

---

### üåü **6. qwen3-coder** - Alibaba's Latest Agentic Code Model

**Tags** : `tools cloud 30b 480b`  
**Pulls** : 526.3K  
**Taille** : 30B (local), 480B (cloud)  
**Description** : "Alibaba's performant long context models for agentic and coding tasks"

**Points Forts** :
- ‚úÖ **Sp√©cialement con√ßu pour t√¢ches agentiques**
- ‚úÖ Long contexte (id√©al pour gros projets)
- ‚úÖ Format tool calling
- ‚ö†Ô∏è Version 30B en local, 480B n√©cessite cloud

**Commandes** :
```bash
ollama pull qwen3-coder:30b    # Version locale
```

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê Si besoin de long contexte

---

### üí™ **7. gpt-oss:20b** - OpenAI's Open Model

**Tags** : `tools thinking 20b 120b`  
**Pulls** : 3.6M  
**Taille** : 20B param√®tres

**Points Forts** :
- ‚úÖ D√©velopp√© par OpenAI (format natif garanti)
- ‚úÖ Support tool calling
- ‚úÖ Bon √©quilibre g√©n√©ral (pas sp√©cialis√© code mais polyvalent)

**Commandes** :
```bash
ollama pull gpt-oss:20b
```

**Recommandation** : ‚≠ê‚≠ê‚≠ê‚≠ê Pour usage polyvalent avec tool calling

---

## üìä Comparaison des Performances Coding

### Benchmarks Standards

| Mod√®le | Taille | Context Max | HumanEval | MBPP | Tool Calling | Licence | Recommandation |
|--------|--------|-------------|-----------|------|--------------|---------|----------------|
| **devstral:24b** | 24B | **128K** üî• | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Natif | Open | üèÜ **TOP CHOIX** |
| **deepseek-coder-v2:16b** | 16B MoE | **160K** üöÄ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Open | ü•à Excellent |
| **qwen2.5-coder:32b** | 32B | **128K** üî• | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Open | ü•â Polyvalent |
| **qwen3-coder:30b** | 30B | **128K** üî• | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Open | üîç Agentique |
| **gpt-oss:20b** | 20B | **128K** üî• | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ | Open | üí° G√©n√©ral |
| **codestral:22b** | 22B | 32K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ö†Ô∏è Restrictive | ‚ö†Ô∏è Licence |
| **deepseek-coder:33b** | 33B | 16K | ‚≠ê‚≠ê‚≠ê‚≠ê (~73%) | ‚≠ê‚≠ê‚≠ê‚≠ê (~76%) | ‚úÖ | Open | ü•â Classique |

### Notes sur les Benchmarks

- **HumanEval** : 164 probl√®mes Python (standard de l'industrie)
- **MBPP** : Mostly Basic Python Problems (suite de HumanEval)
- **Context Max** : Taille maximale de la fen√™tre de contexte (üöÄ = 160K+, üî• = 128K, standard = 16-32K)
- Les scores varient selon la taille du mod√®le et les configurations
- **deepseek-coder-v2:16b** a le plus grand contexte (160K) gr√¢ce √† son architecture MoE

---

## üéØ Recommandation Finale par Cas d'Usage

### üèÜ Pour √âditeur CLI Type Cursor (RECOMMAND√â)

**Choix #1** : **devstral:24b**
- ‚úÖ Sp√©cialement con√ßu pour agents de coding
- ‚úÖ Tool calling natif (format OpenAI)
- ‚úÖ Performance optimale pour d√©veloppement
- ‚úÖ 24B param√®tres (bon √©quilibre)

**Choix #2** : **deepseek-coder-v2:16b**
- ‚úÖ Performance comparable GPT-4 Turbo
- ‚úÖ Architecture MoE efficace
- ‚úÖ Entra√Ænement massif sur code

**Choix #3** : **qwen2.5-coder:14b** ou **32b**
- ‚úÖ Bon √©quilibre taille/performance
- ‚úÖ Tool calling support√©
- ‚úÖ Polyvalent et fiable

---

## üöÄ Plan d'Action Recommand√©

### Phase 1 : Test avec devstral (TOP CHOIX)

```bash
# 1. T√©l√©charger devstral
ollama pull devstral:24b

# 2. Tester rapidement
ollama run devstral:24b "Write a Python function to sort a list"
```

### Phase 2 : Comparer avec deepseek-coder-v2

```bash
# T√©l√©charger alternative
ollama pull deepseek-coder-v2:16b

# Comparer performances sur vos t√¢ches r√©elles
```

### Phase 3 : Migration Code

Utiliser l'API compatible OpenAI d'Ollama :

```typescript
// Dans src/horus/client.ts
constructor(apiKey: string, model?: string, baseURL?: string) {
  this.client = new OpenAI({
    apiKey: apiKey || "ollama",
    baseURL: baseURL || "http://localhost:11434/v1",
    timeout: 360000,
  });
  // Mod√®le par d√©faut
  this.currentModel = model || "devstral:24b";
}
```

```typescript
// Dans src/utils/settings-manager.ts
const DEFAULT_USER_SETTINGS: Partial<UserSettings> = {
  baseURL: "http://localhost:11434/v1",
  defaultModel: "devstral:24b",
  models: [
    "devstral:24b",           // TOP CHOIX
    "deepseek-coder-v2:16b",   // Alternative MoE
    "qwen2.5-coder:14b",       // Option √©quilibr√©e
    "deepseek-coder:33b",       // Classique
  ],
};
```

---

## ‚ö° Optimisations pour Coding

### Options Ollama pour Coding

```typescript
// Options recommand√©es pour g√©n√©ration de code
const options = {
  num_predict: -2,      // Fill context (essentiel!)
  temperature: 0.2,    // Plus d√©terministe pour code (vs 0.7 g√©n√©ral)
  num_ctx: 32768,      // Long contexte pour gros fichiers
};
```

**Pourquoi temp√©rature 0.2 ?**
- Le code n√©cessite pr√©cision, pas cr√©ativit√©
- R√©duit les hallucinations
- Code plus coh√©rent et fonctionnel

---

## üìö Ressources et Benchmarks

### Benchmarks Officiels

- **HumanEval** : https://github.com/openai/human-eval
- **MBPP** : https://github.com/google-research/google-research/tree/master/mbpp
- **CodeXGLUE** : https://github.com/microsoft/CodeXGLUE

### Mod√®les sur Ollama

- **devstral** : https://ollama.com/library/devstral
- **deepseek-coder-v2** : https://ollama.com/library/deepseek-coder-v2
- **qwen2.5-coder** : https://ollama.com/library/qwen2.5-coder
- **deepseek-coder** : https://ollama.com/library/deepseek-coder

## üéâ Conclusion

**Pour un √©diteur CLI de coding type Cursor IDE :**

üèÜ **devstral:24b** est le choix optimal :
- Con√ßu sp√©cifiquement pour agents de coding
- Performance maximale sur t√¢ches de d√©veloppement
- Tool calling natif OpenAI
- 24B param√®tres (bon √©quilibre)

ü•à **deepseek-coder-v2:16b** comme alternative :
- Performance comparable GPT-4 Turbo
- Architecture MoE efficace
- Entra√Ænement massif code

**Complexit√© Migration** : ‚≠ê (Tr√®s simple avec API compatible OpenAI)  
**Temps estim√©** : 30-60 minutes  
**Performance attendue** : √âquivalente ou sup√©rieure √† Grok Code pour t√¢ches de coding

---

*Document g√©n√©r√© le : {{ date }}*

